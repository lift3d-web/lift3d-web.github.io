<!--  -->


<!-- --------------------------------------------------------- -->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="robot learning, diffusion models, 3D vision, imitation learning, deep learning, robotics, manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="628" />
  
  <article class="post-content">
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Lift3D" />
    <meta name="twitter:description"
        content="Lift3D Policy: Lifting 2D Foundation models for Robust 3D Robotic Manipulation" />
    <meta name="twitter:url" content="https://3d-diffuser-actor.github.io/" />
    <meta name="twitter:image" content="https://3d-diffuser-actor.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image" content="https://3d-diffuser-actor.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image:src" content="https://3d-diffuser-actor.github.io/static/images/preview_new.jpeg" />
    <meta name="twitter:image_alt" content="3D Diffuser Actor" />
  <article class="post-content">

  <title>Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="https://fonts.sandbox.google.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />


 <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  

  <style>
    .material-symbols-rounded {
        font-variation-settings: 'FILL' 1,
        'wght' 400,
        'GRAD' 0,
        'opsz' 48
    }


    .linkscontainer {
        max-width: 600px;
        margin: 0 auto;
    }

    .links {
        /*font-size: 5rem;*/
        /*margin: 0 -20px;*/
        display: flex;
        gap: 20px;
        justify-content: center;
        flex-wrap: wrap;
    }

    .links a span.material-symbols-rounded {
        max-width: 25px;
    }

    span.material-symbols-rounded {
        max-width: 25px;
    }

    .links a {
        display: inline-flex;
        /*flex-direction: column;*/
        align-items: center;
        text-decoration: none;
        border-radius: 5px;
        padding: 5px 7px;
        color: #f6f3f1 !important;
        font-family: 'GT Ultra', sans-serif !important;
        font-weight: 400;
        /*margin: 0 10px;*/
        transition: transform .2s;
        background-color: rgba(var(--btn-bgc), 1);
    }


    .links a:nth-child(1) {
        --btn-bgc: 237, 100, 90;
    }

    .links a:hover {
        transform: scale(1.2);
        color: #f6f3f1 !important;
    }

    .links a:active {
        transform: scale(1.3);
    }

    .links a:nth-child(2) {
        --btn-bgc: 47, 138, 196;
    }

    .links a:nth-child(3) {
        --btn-bgc: 229, 134, 6;
    }

    .links a:nth-child(4) {
        --btn-bgc: 133, 180, 51;
    }

    .links a:nth-child(5) {
        --btn-bgc: 204, 97, 176;
    }

    .links a span.material-symbols-rounded {
        margin-right: .25rem;
    }

    .links a:not(:last-child) {
        /*margin-right: 20px;*/
    }


    .links .material-symbols-rounded {
        font-size: 1.25rem;
    }


    .links i {
        font-size: 28px;
    }

    .fig {
        width: 100%;
        display: block;
        padding: 0 10px;
        margin: 0 auto;
    }

    img.arch {
        max-width: 500px;
    }

    img.blobs {
        max-width: 350px;
    }

    .teaser {
        text-align: center;
        margin-bottom: 1rem;
    }

    .teaser + section {
        margin-top: 1rem;
    }

    .teaser video {
        max-width: 600px;
        width: 100%;
    }

    img.teaser {
        max-width: 90%;
    }

    .abstract-imgs .col {
        display: flex;
        align-items: center;
    }

    .videowrapper {
        float: none;
        clear: both;
        width: 100%;
        position: relative;
        padding-bottom: 56.25%;
        padding-top: 25px;
        height: 0;
    }

    .wrapwrap {
        max-width: 800px;
        margin: 0 auto;
    }

    .videowrapper iframe {
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
    }

    .abstract-capt {
        font-size: 0.8rem;
        display: block !important;
        text-align: center;
    }

    .latex {
        display: inline;
        font-family: 'Math', monospace;
        font-style: italic;
    }

    p {
        margin: 0 !important;
    }

    .bs-tooltip-end {
        margin-left: 3px !important;
    }

    section {
        margin: 2rem 0;
    }

    section h5 {
        margin: 1.5rem 0 0.75rem 0;
    }

    .iconbutton {
        padding: 0.1rem 0.3rem;
        display: flex;
        border: none !important;
        background-color: #4B495B !important;
        color: #f5ece5 !important;
        font-family: 'GT Ultra', sans-serif !important;
        font-weight: 400;
        text-decoration: none !important;
    }

    .iconbutton span.material-symbols-rounded {
        font-size: 1.5rem;
        /* font-weight: 700; */
    }

    .playpause span.material-symbols-rounded {
        font-variation-settings: 'wght' 700;


    }

    .iconbutton span.material-symbols-rounded:hover {
        color: #faf6f2 !important;
    }

    .iconbutton:hover, .iconbutton:focus {
        background-color: #2E2E38 !important;
        border-color: none !important;
        color: #faf6f2 !important;
    }

    .iconbutton:focus {

    }

    .iconbutton:active {
        background-color: #09090B !important;

    }

    [data-clipboard-target] {
        cursor: pointer;
    }

    [data-clipboard-target]:hover {
        color: #276FBF !important;
    }

    /* .emptyrooms {
         max-width: 80%;
     }*/

    .emptyrooms {
        display: flex;
        /*justify-content: center;*/
        align-items: flex-start;
        background-color: #f5ece5;
    }

    .playpause {
        flex-shrink: 0;
    }

    .emptyrooms img:first-child {
        width: 16.196944%;
        margin-right: 0.6%;
    }

    .emptyrooms img:last-child {
        width: 82.8862479%;
    }


    section h3 {
        margin-bottom: 1rem;
        display: flex;
        align-items: center;
    }

    section h5, section h3 {
        justify-content: space-between;

        display: flex;
        align-items: center;
    }

    h3, h5 {
        scroll-margin-top: 25px;
        /*display: inline-block;*/
    }

    h3[data-exclude-link], h5[data-exclude-link] {
        cursor: initial;
    }

    h3 span, h5 span {
        display: inline-flex;
        align-items: center;
        cursor: pointer;
    }

    h3 span:hover, h5 span:hover {

        color: #276FBF !important;
    }

    /*
            h3[data-exclude-link]:hover, h5[data-exclude-link]:hover {
                color: initial !important;
            }*/

    .carousel {
        margin: 1rem 0;
    }

    .ltx {
        vertical-align: baseline;
    }

    .cit {
        background-color: rgba(26, 25, 31, 0.05);
        padding: 10px;
        border-radius: 5px;
        font-size: 14px;
        display: inline-block;
        margin: 0 auto;
        overflow-y: hidden;
        overflow-x: auto;
        white-space: pre-wrap;
        white-space: -moz-pre-wrap;
        white-space: -pre-wrap;
        white-space: -o-pre-wrap;
        word-wrap: break-word;
        font-family: 'Code', monospace;
    }

    .cit_cont {
        display: flex;
    }

    .video-container {
        position: relative;
    }

    /* .video-container .video-border {
         position: absolute;
         width: 100%;
         height: 100%;
         top: 0;
         left: 0;
         box-shadow: inset 0px 0px 0px 6px #f5ece5;
     }*/

    .video-container video {
        width: 100%;
        display: block;
        clip-path: inset(5px 5px);
    }

    .video-container img {
        width: 100%;
    }

    .splide > * {
        font-weight: 500;
        position: initial;
    }

    .splide__arrow {
        position: initial;
        background: none;
        /*opacity: 1;*/
        -webkit-transform: none;
        -moz-transform: none;
        -ms-transform: none;
        -o-transform: none;
        transform: none;
        font-size: 1.25rem;
        justify-content: end;
        width: 1.5em;
    }

    .splide__arrow--prev {
        justify-content: left;
    }

    .splide__slide {
        height: 0;
    }

    .splide__slide.is-visible {
        height: auto;
    }

    .move-cont {
        display: flex;
    }

    .move-cont > video {
        max-width: 100%;
    }

    .splide__pagination__page.is-active {
        background: #000;
    }

    .splide__pagination__page {
        -webkit-transition: all 0.3s;
        -moz-transition: all 0.3s;
        -ms-transition: all 0.3s;
        -o-transition: all 0.3s;
        transition: all 0.3s;
    }

    .splide__pagination {
        margin-top: 0.25rem;
    }

    .splide__pagination__page:hover {
        background: #aaa;
        transform: scale(1.2);
    }

    .splide__track_and_arrows {
        display: flex;
    }

    .splide__arrows {
        display: flex;
        align-items: center;
    }

    .styletransfer {
        display: grid;
        grid-row-gap: .5rem;
        grid-template-columns: 1fr 20px 1fr 1fr 1fr 20px 1fr 1fr 1fr;
    }

    .inversion {
        display: grid;
        /*grid-row-gap: .5rem;*/
        /*grid-template-columns: 1fr 20px 1fr 1fr 1fr 20px 1fr 1fr 1fr;*/
        grid-template-columns: 1fr 1fr 1fr 1fr 1fr;
    }

    .styletransfer img {
        max-width: 100%;
        clip-path: inset(2px 2px);
    }

    .inversion span {
        text-align: center;
    }

    .inversion img {
        max-width: 100%;
        clip-path: inset(2px 2px);
    }


    #inversion ~ .splide img {
        max-width: 100%;
        padding: 0.3rem;
    }

    .styletransfer span {
        text-align: center;
    }

    .styletransfer span:nth-child(2) {
        grid-column-start: 3;
        grid-column-end: 6;
    }

    .styletransfer span:nth-child(3) {
        grid-column-start: 7;
        grid-column-end: 10;
    }


    @media (min-width: 992px) {
        .container {
            max-width: 1050px;
        }
    }

    .inversion_subheader {
        font-weight: bold;
        text-align: center;
        margin-top: 0.5rem;
    }

    .inversion_subheader:not(:first-of-type) {
        margin-top: 0.2rem;
    }

    .inversion_subheader + .splide {
        margin: 0.2rem 0 !important;
    }

    /*
            .move-cont .video-container:nth-child(8), .move-cont .video-container:nth-child(7) {
                display: none;
            }*/

    @media (max-width: 991px) {
        /*
                    .move-cont .video-container:nth-child(5), .move-cont .video-container:nth-child(6) {
                        display: none;
                    }*/
        .move-cont > video {
            max-width: 150%;
            clip-path: inset(0 33.333333% 0 0);
        }

        header h3 {
            font-size: 1.5rem;
        }

        .cit {
            /*font-size: 12px !important;*/
        }

        .emptyrooms img:last-child {
            width: 99.6% !important;
        }

        .emptyrooms img:first-child {
            width: 19.46% !important;
            margin-right: 0.7522% !important;
        }

        .styletransfer {
            display: grid;
            grid-template-columns: 1fr 15px 1fr 1fr 15px 1fr 1fr !important;
        }

        .styletransfer img:nth-child(9n+3), .styletransfer img:nth-child(9n-1) {
            display: none;
        }

        .styletransfer span:nth-child(2) {
            grid-column-start: 3;
            grid-column-end: 5;
        }

        .styletransfer span:nth-child(3) {
            grid-column-start: 6;
            grid-column-end: 8;
        }
    }

    .author {
        padding: 0 0.6rem;
    }

    @media (max-width: 767px) {
        html, body {

            /*font-size: 16px;*/
        }

        .author {
            font-size: 18px;
        }

        .insts {
            font-size: 16px;
        }

        /*.move-cont .video-container:nth-child(4) {
            display: none;
        }*/
        .move-cont > video {
            max-width: 200%;
            clip-path: inset(0 50% 0 0);
        }

        .inversion *:nth-child(5n+2) {
            display: none;
        }

        .inversion {
            grid-template-columns: 1fr 1fr 1fr 1fr;
        }

        .emptyrooms img:last-child {
            width: 124.477307% !important;
        }

        .emptyrooms img:first-child {
            width: 24.3243243% !important;
            margin-right: 0.9% !important;
        }

        .styletransfer {
            display: grid;
            grid-template-columns: 1fr 10px 1fr 10px 1fr !important;
        }

        .styletransfer img:nth-child(9n+2), .styletransfer img:nth-child(9n-2) {
            display: none;
        }

        .styletransfer span:nth-child(2) {
            grid-column-start: 3;
            grid-column-end: 4;
        }

        .styletransfer span:nth-child(3) {
            grid-column-start: 5;
            grid-column-end: 6;
        }
    }

    @media (max-width: 540px) {
        .links a span.material-symbols-rounded {
            max-width: 20px;
        }

        .links {
            gap: 12px;
        }
    }

    @media (max-width: 510px) {
        /*
                    .move-cont .video-container:nth-child(3) {
                        display: none;
                    }*/
        .move-cont > video {
            max-width: 300%;
            clip-path: inset(0 66.6666666% 0 0);
        }


        .inversion {
            grid-template-columns: 1fr 1fr;
        }

        .inversion span:nth-child(n+4) {
            grid-row: 3;
        }

        .links .material-symbols-rounded {
            /*display: none;*/
            font-size: 1rem;
        }

        .links a span.material-symbols-rounded {
            max-width: 18px;
        }

    }

    @media (max-width: 399px) {

    }

    @media (max-width: 380px) {
        /*.links .material-symbols-rounded {*/
        /*    display: none;*/
        /*    padding: 5px 10px;*/
        /*    !*font-size: 1rem;*!*/
        /*}*/
    }

    @media (max-width: 575px) {

        img.blobs {
            max-width: 300px;
        }

        .emptyrooms img:last-child {
            width: 165.941536% !important;
        }

        .emptyrooms img:first-child {
            width: 32.4269205% !important;
            margin-right: 1.15% !important;
        }
    }

    @media (min-width: 768px) {
        .abstract-imgs .col-md-7 {
            border-right: 1px solid #1f1e1d;
        }

        img.arch, img.blobs {
            max-width: 600px;
        }

    }


</style>
</head>
<!-- --------------------------------------------------------- -->
<body>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Lift3D Foundation Policy: Lifting 2D Large-Scale Pretrained Models for Robust 3D Robotic Manipulation</h1>
              <div class="is-size-4 publication-authors">
                <span class="author-block">
                    <a href="https://github.com/jiayueru">Yueru Jia</a><sup>1*</sup>, 
                    <a href="https://liujiaming1996.github.io/">Jiaming Liu</a><sup>1*†</sup>, 
                    <a href="https://lift3d-web.github.io/">Sixiang Chen</a><sup>1*</sup>, 
                    <a href="https://gaystarc.github.io/">Chenyang Gu</a><sup>1</sup>, 
                    <a href="https://lift3d-web.github.io/">Zhilue Wang</a><sup>1</sup>, 
                    <a href="https://luolongzan.github.io/">Longzan Luo</a><sup>1</sup>, 
                    <a href="https://lift3d-web.github.io/">Lily Lee</a><sup>1</sup>, 
                    <a href="https://lift3d-web.github.io/">Pengwei Wang</a><sup>2</sup>, 
                    <a href="https://lift3d-web.github.io/">Renrui Zhang</a><sup>†</sup>, 
                    <a href="https://lift3d-web.github.io/">Zhongyuan Wang</a><sup>2</sup>, 
                    <a href="https://lift3d-web.github.io/">Shanghang Zhang</a><sup>1,2✉</sup>
                </span>
                <br>
                <span class="affiliation-block">
                    <sup>1</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University; 
                    <sup>2</sup>Beijing Academy of Artificial Intelligence (BAAI)
                </span>
                <br>
                <span class="note-block">
                    <em>* Equal contribution, † Project lead, ✉ Corresponding author</em>
                </span>
    
              <!-- <table align=center width=700px>
                <tr>
                  <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2304.14391">[Paper]</a></span></center></td>
                  <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/nickgkan/butd_detr">[Code]</a></span></center></td>
              <tr/>
            </table> -->
    
            <div class="linkscontainer">
              <div class="links mt-4">
              <a href="" target="_blank"><span class="material-symbols-rounded">
              description
              </span><span>Paper</span></a>
              <a href="" target="_blank"><span class="material-symbols-rounded">
              code
              </span><span>Code</span></a>


              <!-- <a href="#results"><span class="material-symbols-rounded">
              science
              </span><span>Results</span></a> -->
              </div>
              </div>
    
            </div>
          </div>
        </div>
      </div>
    </section>





    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Lift3D Policy</h2>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <video id="put_grapes" autoplay controls muted loop playsinline width="100%">
                <source src="./static/videos/arxiv.mov"
                        type="video/mp4">
            </video>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                    3D geometric information is essential for manipulation tasks, as robots need to perceive the 3D environment, reason about spatial relationships, and interact with intricate spatial configurations. Recent research has increasingly focused on the explicit extraction of 3D features, while still facing challenges such as the lack of large-scale robotic 3D data and the potential loss of spatial geometry. To address these limitations, we propose the Lift3D framework, which progressively enhances 2D foundation models with implicit and explicit 3D robotic representations to construct a robust 3D manipulation policy. Specifically, we first design a task-aware masked autoencoder that masks task-relevant affordance patches and reconstructs depth information, enhancing the 2D foundation model’s implicit 3D robotic representation. After self-supervised fine-tuning, we introduce a 2D model-lifting strategy that establishes a positional mapping between the input 3D points and the positional embeddings of the 2D model. Based on the mapping, Lift3D utilizes the 2D foundation model to directly encode point cloud data, leveraging large-scale pretrained knowledge to construct explicit 3D robotic representations while minimizing spatial information loss. In experiments, Lift3D consistently outperforms previous state-of-the-art methods across several simulation benchmarks and real-world scenarios. 
                    </p>
                </div>
            </div>
          </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Overview</h2>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <img src="./static/images/overview.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <div class="content has-text-justified">
                    <p>
                    Lift3D empowers 2D foundation models with 3D manipulation capabilities by refining implicit 3D robotic representations through task-related affordance masking and depth reconstruction, while enhancing explicit 3D robotic representations by leveraging the pretrained 2D positional embeddings to encode point cloud. Lift3D achieves robustness and surprising effectiveness in diverse simulation and real-world tasks.
                    </p>
                </div>
            </div>
          </div>
        </div>
    </section>


    <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Lift3D pipeline</h2>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <img src="./static/images/arch.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="100%"/>
            </div>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column">
                <div class="content has-text-justified">
                    <p>
                    For implicit 3D robotic representation, we leverage CLIP to offline extract image attention maps based on task descriptions, which are back-projected onto the 2D input to guide the MAE masking. We then input the visible tokens into the 2D foundation model to extract features. The masked tokens and encoded visible tokens are processed by the MAE decoder for depth reconstruction, enhancing 3D spatial awareness. Meanwhile, the encoded visible tokens are also distilled using corresponding features from the off-the-shelf pretrained model to mitigate catastrophic forgetting.
                    </p>
                    <p>
                    For explicit 3D robotic representation, we first project the point cloud data onto multiple virtual planes, establishing a positional mapping between the 3D input points and the 2D positional embeddings (PEs) on each virtual plane. After mapping, we average the 2D PEs corresponding to each 3D patch to form a unified positional indicator(3D PEs), which is then integrated with the 3D tokens.These 3D tokens are generated by feeding the point cloud into a lightweight 3D tokenizer. Finally, the output features from the 2D foundation model are processed through a policy head to predict the pose for imitation learning.
                    </p>
                </div>
            </div>
          </div>
        </div>
    </section>

    <!--<section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <h2 class="title is-3">Visualization of the pose denoising process</h2>
          </div>
          <div class="columns is-centered has-text-centered">
      
            <div class="column">
              <div class="content has-text-justified">
                <p>
                We show 60 samples for the next predicted trajectory on CALVIN.
                </p>
              </div>
              <div class="content has-text-centered">
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_calvin.mp4"
                          type="video/mp4">
                </video>
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_calvin1.mp4"
                          type="video/mp4">
                </video>
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_calvin2.mp4"
                          type="video/mp4">
                </video>
              </div>
              <div class="content has-text-justified">
                <p>
                We show 60 samples for the next predicted keypose in the real-world.
                3D Diffuser Actor <b>captures all modes of equivalent</b> behaviors for the different tasks: 3 modes for "picking up the grape", 2 modes for "inserting the peg in a hole" and 2 modes for "finding the mouse".
                </p>
              </div>
              <div class="content has-text-centered">
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_realworld.mp4"
                          type="video/mp4">
                </video>
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_realworld2.mp4"
                          type="video/mp4">
                </video>
                <video id="put_grapes" autoplay controls muted loop playsinline width="32%">
                  <source src="./static/videos/denoising_realworld3.mp4"
                          type="video/mp4">
                </video>
              </div>
            </div>
      
          </div>
        </div>
    </section>

    <span style="display:block; margin-top:-1.75em;"/>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">State-of-the-art on CALVIN</h2>
                    <div class="content has-text-justified">
                        <p>
                        We test on CALVIN on the setting of <b>zero-shot unseen-scene generalization</b>. All models are trained on environments A, B, C and tested on environment D. 3D Diffuser Actor outperforms prior arts by a large margin, achieving 0.2 more sequential tasks, a <b>7% relative improvement</b>.
                        </p>
                    </div>
                    <div class="content has-text-centered">
                        <img src="./static/images/sota_calvin.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="65%"/>
                    </div>
                    <div class="content has-text-centered">
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq133.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq150.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq29.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq73.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq79.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/calvin/new/new_calvin_seq94.mp4"
                                type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section> -->

    <!--<section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">State-of-the-art on RLBench</h2>
                    <div class="content has-text-justified">
                        <p>
                        We test on a multi-task setup of 18 manipulation tasks on RLBench. All models use 4 camera views and 100 expert demonstrations for each task. 3D Diffuser Actor outperforms prior arts by a large margin, achieving <b>16.0 absolute performance gain</b> on average across tasks.
                        </p>
                    </div>

                    <div class="content has-text-centered">
                        <img src="./static/images/sota_rlbench.png" alt="input image" style="vertical-align:middle;margin:0px 0px" width="65%"/>
                    </div>

                    <div class="content has-text-centered">
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_hang_cups.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_sort_shape.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_stack_cups.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_stack_blocks.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_put_in_cupboard.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_insert_peg.mp4"
                                type="video/mp4">
                        </video>

                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_close_jar.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_drag_block.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_open_drawer.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_place_wine.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_put_block_in_drawer.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_put_cash_in_safe.mp4"
                                type="video/mp4">
                        </video>

                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_screw_lightbulb.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_slide_block.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_sweep_dust.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_take_meat_off_grill.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_touch_button.mp4"
                                type="video/mp4">
                        </video>
                        <video id="put_grapes" autoplay controls muted loop playsinline width="16%">
                        <source src="./static/videos/rlbench/new/new_turn_tap.mp4"
                                type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>-->


    <!--<footer class="footer">
        <div class="container">
            <div class="content has-text-centered">

                <a class="icon-link" href="https://arxiv.org/abs/2402.10885" target="_blank">
                    <i class="ai ai-arxiv"></i>
                </a>
                &nbsp;

                <a class="icon-link" href="https://arxiv.org/pdf/2402.10885.pdf" target="_blank">
                    <i class="fas fa-file-pdf"></i>
                </a>
                &nbsp;
                <a class="icon-link" href="https://github.com/nickgkan/3d_diffuser_actor"
                    target="_blank">
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        Page source code was adapted from
                        <a href="https://nerfies.github.io" target="_blank">here</a>
                        and
                        <a href="https://ebmplanner.github.io/"
                            target="_blank">here</a>,
                        and can be found in <a
                            href="https://github.com/3d-diffuser-actor/3d-diffuser-actor.github.io"
                            target="_blank">this repository</a>.
                    </p>
                </div>
            </div>
    </footer> -->

    <!-- <script src="./static/js/index.js"></script>
    <script src="./static/js/prism.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.js"
        integrity="sha256-+dK6uqUp/DnP6ef97s8XcoynBnGe5vM5gvBECH0EB3U=" crossorigin="anonymous">
        </script> -->

        <section class="section">
            <!-- Zero Shot. -->
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column">
                        <h2 class="title is-3">Lift3D in the real world</h2>
                        <div class="content has-text-justified">
                            <p>
                                In our real-world setup, we conduct experiments using a Franka Research 3 arm, with a static front view captured by an Intel RealSense L515 RGBD camera. We perform ten tasks: place bottle at rack, pour water, unplug charger, stack blocks, pick and place, slide block, water plants, wipe table, open drawer, and close drawer. These tasks involve various types of interacted objects and manipulation actions. For each task, 40 demonstrations are collected in diverse spatial positions, with trajectories recorded at 30 fps. We select 30 episodes and extract key frames to construct the training set for each task. 
                            </p>
                        </div>
    
                        <div class="content has-text-centered">
                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/w.mp4"
                                    type="video/mp4">
                            </video>

                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/2.mp4"
                                    type="video/mp4">
                            </video>

                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/c.mp4"
                                    type="video/mp4">
                            </video>

                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/p.mp4"
                                    type="video/mp4">
                            </video>
    
                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/r.mp4"
                                    type="video/mp4">
                            </video>

                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                            <source src="./static/videos/pandp.mp4"
                                    type="video/mp4">
                            </video>

                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                                <source src="./static/videos/close.mp4"
                                        type="video/mp4">
                            </video>
                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                                <source src="./static/videos/open.mp4"
                                        type="video/mp4">
                            </video>
                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                                <source src="./static/videos/stack.mp4"
                                        type="video/mp4">
                            </video>     
                            <video id="put_grapes" autoplay controls muted loop playsinline width="19%">
                                <source src="./static/videos/water.mp4"
                                        type="video/mp4">
                            </video>                                                       
                        </div>
                    </div>
                </div>
            </div>
        </section>
</body>

</html>
